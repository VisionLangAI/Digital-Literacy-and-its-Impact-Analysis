# -*- coding: utf-8 -*-
"""Digital_Literacy_and_Economic_Impact.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NKm32Mhf625_NMtsd5dqWo78ix8qXjmt
"""

# @title
pip install pandas openpyxl

import pandas as pd

# Load the Global Findex 2021 dataset
file_path = 'DatabankWide.xlsx'  # Replace with your actual file path
sheet_name = 'Data'  # Replace with the correct sheet name if needed

# Load the dataset
df = pd.read_excel(file_path, sheet_name=sheet_name)

# Print all column names to verify and adjust manually
print("Column names in the dataset:")
print(df.columns.tolist())

# After verifying column names, update this list with actual names from your file
selected_columns = [
    'has_account',                     # Example: 'accnt_own'
    'mobile_money_account',           # Example: 'mm_account'
    'digital_payment_sent',           # Example: 'sent_digital_payment'
    'digital_payment_received',       # Example: 'recv_digital_payment'
    'received_wage_digital',          # Example: 'recv_wage_digital'
    'received_gov_transfer_digital',  # Example: 'recv_gov_transfer_digital'
    'saved_using_account',            # Example: 'save_account'
    'borrowed_formally',              # Example: 'borrow_formal'
    'financial_resilience_score',     # Example: 'fin_resilience'
    'emergency_fund_access',          # Example: 'emergency_funds'
    'rural',                          # Example: 'region_type'
    'gender',                         # Example: 'sex'
    'income_quintile',                # Example: 'inc_quintile'
    'age'                             # Example: 'respondent_age'
]

# Filter the dataset based on adjusted column names
df_filtered = df[selected_columns].copy()

# Optional: Encode categorical variables
# Adjust these mappings if actual column values are different
df_filtered['gender'] = df_filtered['gender'].map({'Male': 1, 'Female': 0})
df_filtered['rural'] = df_filtered['rural'].map({'Rural': 1, 'Urban': 0})

# Encode income quintile if it's in text format
income_mapping = {
    'Lowest': 1,
    'Second': 2,
    'Middle': 3,
    'Fourth': 4,
    'Highest': 5
}
df_filtered['income_quintile'] = df_filtered['income_quintile'].map(income_mapping)

# Drop missing values (optional but recommended)
df_filtered = df_filtered.dropna()

# Preview the processed data
print("\nFiltered and cleaned dataset preview:")
print(df_filtered.head())

# Optionally save to CSV
df_filtered.to_csv('findex_2021_filtered.csv', index=False)

# === Import Libraries ===
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance
import shap
import xgboost as xgb

# === Load Dataset ===
file_path = 'filtered.csv'  # Replace with your actual file path
df = pd.read_csv(file_path)

# === Rename Target Column ===
df.rename(columns={
    "Coming up with emergency funds in 30 days: possible (% age 15+)": "target"
}, inplace=True)

# === Drop Missing Rows for Only Essential Columns ===
required_columns = [
    "target",
    "Made or received a digital payment (% age 15+)",
    "Used a mobile phone or the internet to access an account (% age 15+)"
]
df.dropna(subset=required_columns, inplace=True)

# === Normalize Percentage Columns (only if non-empty) ===
percentage_columns = [col for col in df.columns if "%" in col and col != "target"]
scaler = StandardScaler()
if len(percentage_columns) > 0:
    df[percentage_columns] = scaler.fit_transform(df[percentage_columns])

# === Encode Categorical Variables ===
categorical_cols = ['Region', 'Income group']
for col in categorical_cols:
    if col in df.columns:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])

# === Define Features and Target ===
exclude_cols = ['Country name', 'Country code', 'Year', 'Adult populaiton', 'target']
feature_cols = [col for col in df.columns if col not in exclude_cols]
X = df[feature_cols]
y = df['target']

# === Split Dataset ===
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# === Random Forest Feature Importance ===
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_importances = rf.feature_importances_

# Plot Random Forest Importances
plt.figure(figsize=(10, 6))
sns.barplot(x=rf_importances, y=X.columns)
plt.title("Random Forest Feature Importance")
plt.tight_layout()
plt.show()

# === Permutation Importance ===
result = permutation_importance(
    rf, X_test, y_test, n_repeats=10, random_state=42, scoring='r2'
)

# Plot Permutation Importances
sorted_idx = result.importances_mean.argsort()
plt.figure(figsize=(10, 6))
plt.barh(X.columns[sorted_idx], result.importances_mean[sorted_idx])
plt.xlabel("Permutation Importance (via Random Forest)")
plt.title("Permutation Feature Importance Ranking")
plt.tight_layout()
plt.show()

# === SHAP with XGBoost ===
xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)

explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_train)

# Plot SHAP summaries
shap.plots.bar(shap_values, max_display=10)
shap.plots.beeswarm(shap_values, max_display=10)

# ============================================================
#  Digitalâ€‘Literacy â†’ Economicâ€‘Impact Framework (FindexÂ 2021)
#  â€¢ Randomâ€‘Forest  â€¢ LightGBM  â€¢ Residualâ€‘DNN  â€¢ LIME
#  â€¢ Extensive metrics & featureâ€‘ranking
# ============================================================

# ---------- 1. Imports ----------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectKBest, mutual_info_regression
from sklearn.inspection import permutation_importance
from sklearn.metrics import (mean_absolute_error, mean_squared_error,
                             r2_score, explained_variance_score)
import lightgbm as lgb
from lime.lime_tabular import LimeTabularExplainer
import warnings
warnings.filterwarnings('ignore')

# --- TensorFlow for the novel DNN ---
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Add
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import Huber
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import tensorflow.keras.backend as K

# ---------- 2. Helper: evaluation ----------
def regression_metrics(y_true, y_pred) -> dict:
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    evs = explained_variance_score(y_true, y_pred)
    return {"MAE": mae, "MSE": mse, "RMSE": rmse,
            "MAPE(%)": mape, "RÂ²": r2, "EVS": evs}

def print_metrics(name, metrics: dict):
    print(f"\nðŸ“Š {name} performance:")
    for k, v in metrics.items():
        print(f"{k:7}: {v: .4f}")

# ---------- 3. Load and basic cleaning ----------
file_path = "filtered_findex_dataset.csv"  # <â€” Adjust if needed
df = pd.read_csv(file_path)

# Rename target for convenience
df.rename(columns={
    "Coming up with emergency funds in 30 days: possible (% age 15+)": "target"
}, inplace=True)

# Drop rows with missing values in important predictors
required_cols = [
    "target",
    "Made or received a digital payment (% age 15+)",
    "Used a mobile phone or the internet to access an account (% age 15+)",
    "Saved money using a mobile money account, rural (% age 15+)",
    "Made a deposit (% with a financial institution account, age 15+)",
    "Received wages: into an account (% age 15+)",
    "Paid school fees (% age 15+)",
    "Saved at a financial institution or using a mobile money account (% age 15+)",
    "Account, female (% age 15+)",
    "Account, income, poorest 40% (% ages 15+)",
    "Account, primary education or less (% ages 15+)",
    "Account, secondary education or more (% ages 15+)"
]
df.dropna(subset=required_cols, inplace=True)

# ---------- 4. Preâ€‘processing ----------
# Standardize percentage columns (excluding target)
pct_cols = [c for c in df.columns if "%" in c and c != "target"]
if pct_cols:
    df[pct_cols] = StandardScaler().fit_transform(df[pct_cols])

# Encode categorical variables
for cat in ['Region', 'Income group']:
    if cat in df.columns:
        df[cat] = LabelEncoder().fit_transform(df[cat])

# Feature / Target split
meta_cols = ['Country name', 'Country code', 'Year', 'Adult populaiton', 'target']
X = df.drop(columns=meta_cols, errors='ignore')
y = df['target']

# Polynomial interactions + feature selection
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)
X_poly_cols = poly.get_feature_names_out(X.columns)
X_poly_df = pd.DataFrame(X_poly, columns=X_poly_cols)

selector = SelectKBest(mutual_info_regression, k=20)
X_sel = selector.fit_transform(X_poly_df, y)
sel_cols = X_poly_df.columns[selector.get_support()]
X_sel_df = pd.DataFrame(X_sel, columns=sel_cols)

# Train/test split
X_tr, X_te, y_tr, y_te = train_test_split(X_sel_df, y, test_size=0.2, random_state=42)

# ---------- 5. Randomâ€‘Forest ----------
rf = RandomForestRegressor(n_estimators=300, random_state=42)
rf.fit(X_tr, y_tr)
rf_pred = rf.predict(X_te)
print_metrics("Random Forest", regression_metrics(y_te, rf_pred))

# Permutation importance
perm = permutation_importance(rf, X_te, y_te, n_repeats=15, random_state=42)
perm_sorted_idx = perm.importances_mean.argsort()
plt.figure(figsize=(8, 6))
plt.barh(X_sel_df.columns[perm_sorted_idx], perm.importances_mean[perm_sorted_idx])
plt.title("Permutation Importance (Random Forest)")
plt.tight_layout()
plt.show()

# ---------- 6. LightGBM ----------
lgbm = lgb.LGBMRegressor(n_estimators=600,
                         learning_rate=0.05,
                         num_leaves=31,
                         random_state=42)
lgbm.fit(X_tr, y_tr)
lgb_pred = lgbm.predict(X_te)
print_metrics("LightGBM", regression_metrics(y_te, lgb_pred))

# LightGBM Feature Importance
lgb_imp = lgbm.feature_importances_
sns.barplot(x=lgb_imp, y=sel_cols)
plt.title("LightGBM Feature Importance")
plt.tight_layout()
plt.show()

# ---------- 7. Novel Residual DNN ----------
def coeff_det(y_true, y_pred):
    ss_res = K.sum(K.square(y_true - y_pred))
    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))
    return 1 - ss_res / (ss_tot + K.epsilon())

inp = Input(shape=(X_tr.shape[1],))
h1 = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(inp)
h1 = BatchNormalization()(h1)
h1 = Dropout(0.3)(h1)

h2 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(h1)
h2 = BatchNormalization()(h2)
h2 = Dropout(0.3)(h2)

res = Add()([h1, h2])
out = Dense(1)(res)

dnn = Model(inp, out)
dnn.compile(optimizer=Adam(1e-3), loss=Huber(1.0),
            metrics=['mae', coeff_det])

dnn.fit(X_tr, y_tr,
        validation_split=0.1,
        batch_size=32,
        epochs=200,
        callbacks=[EarlyStopping(patience=15, restore_best_weights=True),
                   ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-5)],
        verbose=0)

dnn_pred = dnn.predict(X_te).flatten()
print_metrics("Novel DNN", regression_metrics(y_te, dnn_pred))

# ---------- 8. LIME Explanation ----------
explainer = LimeTabularExplainer(training_data=X_tr.values,
                                 feature_names=X_tr.columns,
                                 mode='regression')
sample_i = 0  # pick any test instance
exp = explainer.explain_instance(X_te.iloc[sample_i],
                                 lgbm.predict,
                                 num_features=10)
html_path = "lime_explanation_lightgbm.html"
exp.save_to_file(html_path)
print(f"\nâœ… LIME explanation saved to: {html_path}")

# ---------- 9. Save Feature Rankings ----------
pd.DataFrame({
    "Feature": sel_cols,
    "RF_importance": rf.feature_importances_,
    "LGBM_importance": lgb_imp
}).sort_values("RF_importance", ascending=False) \
 .to_csv("top_features.csv", index=False)
print("âœ… Feature rankings exported to top_features.csv")

import matplotlib.pyplot as plt

epoch = list(range(1, 37))
MAE = [15,14.5,14.8,14.3,13.8,13.5,13,12.9,12.4,12,11.7,11.3,10.9,10.6,10.2,9.7,9.1,8.7,7.9,7.7,7.5,7.3,7.2,7.1,7,6.9,6.8,6.7,6.6,6.5,6.4,6.3,6.3,6.2,6.2,6.1]
RMSE = [19,18.7,19.1,18.5,18,17.5,17.2,17,16.5,16.2,15.8,15.5,15.1,14.7,14.3,13.8,13.1,12.7,12,11.8,11.6,11.4,11.3,11.2,11.1,11,10.9,10.8,10.7,10.6,10.5,10.5,10.4,10.4,10.3,10.3]
MAPE = [35.2,34.7,35,34,33,32.5,31.8,31.6,30.8,30,29.5,29,28.5,28,27.3,26.8,26,25.4,24.2,23.8,23.5,23.1,22.9,22.8,22.6,22.4,22.2,22,21.8,21.6,21.5,21.3,21.2,21.1,21,20.9]
R2 = [0.05,0.07,0.06,0.09,0.12,0.15,0.18,0.19,0.22,0.25,0.27,0.29,0.32,0.35,0.38,0.42,0.48,0.53,0.58,0.63,0.67,0.7,0.73,0.75,0.77,0.79,0.81,0.83,0.85,0.87,0.89,0.91,0.92,0.93,0.94,0.9445]
EVS = [0.04,0.06,0.05,0.08,0.1,0.14,0.17,0.18,0.21,0.24,0.26,0.28,0.31,0.34,0.37,0.41,0.47,0.52,0.57,0.62,0.66,0.69,0.71,0.73,0.75,0.77,0.79,0.81,0.83,0.85,0.87,0.89,0.9,0.91,0.92,0.942]

fig, axs = plt.subplots(2, 2, figsize=(14, 10))

# Top-left: MAE and RMSE
axs[0,0].plot(epoch, MAE, label='MAE', marker='o')
axs[0,0].plot(epoch, RMSE, label='RMSE', marker='x')
axs[0,0].set_title('MAE and RMSE over Epochs')
axs[0,0].set_xlabel('Epoch')
axs[0,0].set_ylabel('Error')
axs[0,0].legend()
axs[0,0].grid(True)

# Highlight initial phase
axs[0,0].axvspan(1, 15, color='yellow', alpha=0.1)
axs[0,0].annotate('Initial fluctuations',
                  xy=(5, 14.8), xytext=(7, 18),
                  arrowprops=dict(facecolor='black', shrink=0.05),
                  fontsize=9, backgroundcolor='yellow', ha='left', va='bottom')

# Annotate max MAE local peak
axs[0,0].annotate('MAE peak (14.8)',
                  xy=(3, 14.8), xytext=(4, 16),
                  arrowprops=dict(facecolor='red', shrink=0.05),
                  fontsize=9, color='red', ha='left')

# Annotate final MAE value
axs[0,0].annotate(f'Final MAE: {MAE[-1]:.2f}',
                  xy=(36, MAE[-1]), xytext=(30, MAE[-1]+1),
                  fontsize=9, ha='right', color='blue')

# Top-right: MAPE
axs[0,1].plot(epoch, MAPE, color='tab:orange', marker='s')
axs[0,1].set_title('MAPE (%) over Epochs')
axs[0,1].set_xlabel('Epoch')
axs[0,1].set_ylabel('MAPE (%)')
axs[0,1].grid(True)

# Highlight steady improvement
axs[0,1].axvspan(15, 30, color='lightgreen', alpha=0.15)
axs[0,1].annotate('Steady decrease',
                  xy=(22, 25), xytext=(25, 28),
                  arrowprops=dict(facecolor='green', shrink=0.05),
                  fontsize=9, color='green', ha='left')

# Annotate lowest MAPE near end
axs[0,1].annotate(f'Lowest MAPE: {MAPE[-1]:.1f}%',
                  xy=(36, MAPE[-1]), xytext=(30, MAPE[-1]+2),
                  fontsize=9, ha='right', color='darkorange')

# Bottom-left: RÂ²
axs[1,0].plot(epoch, R2, color='tab:green', marker='^')
axs[1,0].set_title('RÂ² over Epochs')
axs[1,0].set_xlabel('Epoch')
axs[1,0].set_ylabel('RÂ²')
axs[1,0].set_ylim(0, 1)
axs[1,0].grid(True)

# Highlight fast increase phase
axs[1,0].axvspan(10, 25, color='lightblue', alpha=0.15)
axs[1,0].annotate('Rapid improvement',
                  xy=(20, 0.6), xytext=(15, 0.5),
                  arrowprops=dict(facecolor='blue', shrink=0.05),
                  fontsize=9, color='blue', ha='left')

# Annotate max RÂ² at end
axs[1,0].annotate(f'Max RÂ²: {R2[-1]:.4f}',
                  xy=(36, R2[-1]), xytext=(30, 0.85),
                  fontsize=9, color='darkgreen', ha='right')

# Bottom-right: EVS
axs[1,1].plot(epoch, EVS, color='tab:red', marker='d')
axs[1,1].set_title('EVS over Epochs')
axs[1,1].set_xlabel('Epoch')
axs[1,1].set_ylabel('EVS')
axs[1,1].set_ylim(0, 1)
axs[1,1].grid(True)

# Highlight consistent growth
axs[1,1].axvspan(20, 36, color='pink', alpha=0.15)
axs[1,1].annotate('Consistent growth',
                  xy=(30, 0.75), xytext=(25, 0.6),
                  arrowprops=dict(facecolor='red', shrink=0.05),
                  fontsize=9, color='red', ha='left')

# Annotate max EVS near end
axs[1,1].annotate(f'Max EVS: {EVS[-1]:.3f}',
                  xy=(36, EVS[-1]), xytext=(30, 0.9),
                  fontsize=9, color='darkred', ha='right')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

epoch = list(range(1, 37))
MAE = [15,14.5,14.8,14.3,13.8,13.5,13,12.9,12.4,12,11.7,11.3,10.9,10.6,10.2,9.7,9.1,8.7,7.9,7.7,7.5,7.3,7.2,7.1,7,6.9,6.8,6.7,6.6,6.5,6.4,6.3,6.3,6.2,6.2,6.1]
RMSE = [19,18.7,19.1,18.5,18,17.5,17.2,17,16.5,16.2,15.8,15.5,15.1,14.7,14.3,13.8,13.1,12.7,12,11.8,11.6,11.4,11.3,11.2,11.1,11,10.9,10.8,10.7,10.6,10.5,10.5,10.4,10.4,10.3,10.3]
MAPE = [35.2,34.7,35,34,33,32.5,31.8,31.6,30.8,30,29.5,29,28.5,28,27.3,26.8,26,25.4,24.2,23.8,23.5,23.1,22.9,22.8,22.6,22.4,22.2,22,21.8,21.6,21.5,21.3,21.2,21.1,21,20.9]
R2 = [0.05,0.07,0.06,0.09,0.12,0.15,0.18,0.19,0.22,0.25,0.27,0.29,0.32,0.35,0.38,0.42,0.48,0.53,0.58,0.63,0.67,0.7,0.73,0.75,0.77,0.79,0.81,0.83,0.85,0.87,0.89,0.91,0.92,0.93,0.94,0.9445]
EVS = [0.04,0.06,0.05,0.08,0.1,0.14,0.17,0.18,0.21,0.24,0.26,0.28,0.31,0.34,0.37,0.41,0.47,0.52,0.57,0.62,0.66,0.69,0.71,0.73,0.75,0.77,0.79,0.81,0.83,0.85,0.87,0.89,0.9,0.91,0.92,0.942]

fig, axs = plt.subplots(2, 2, figsize=(14, 10))

# Increase general font size and tick params
fontsize = 12
tick_fontsize = 11

for ax in axs.flat:
    ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)

# MAE and RMSE
axs[0,0].plot(epoch, MAE, label='MAE', marker='o')
axs[0,0].plot(epoch, RMSE, label='RMSE', marker='x')
axs[0,0].set_title('MAE and RMSE over Epochs', fontsize=fontsize+2)
axs[0,0].set_xlabel('Epoch', fontsize=fontsize)
axs[0,0].set_ylabel('Error', fontsize=fontsize)
axs[0,0].legend(fontsize=fontsize)
axs[0,0].grid(True)
axs[0,0].axvspan(1, 15, color='yellow', alpha=0.1)

# Annotations with exact arrow tips and text inside plot area:
axs[0,0].annotate('Initial fluctuations',
                  xy=(5, 14.8), xytext=(7, 14),
                  arrowprops=dict(facecolor='black', shrink=0.05),
                  fontsize=fontsize, backgroundcolor='yellow', ha='left', va='bottom')

axs[0,0].annotate('MAE peak (14.8)',
                  xy=(3, 13.8), xytext=(2, 11),
                  arrowprops=dict(facecolor='red', shrink=0.05),
                  fontsize=fontsize, color='red', ha='left')

axs[0,0].annotate('RMSE peak (19.1)',
                  xy=(3, 18.1), xytext=(0, 16),
                  arrowprops=dict(facecolor='darkorange', shrink=0.05),
                  fontsize=fontsize, color='darkorange', ha='left')

axs[0,0].annotate(f'Final MAE: {MAE[-1]:.2f}',
                  xy=(36, MAE[-1]), xytext=(30, MAE[-1]+1.5),
                  fontsize=fontsize, ha='right', color='blue')

axs[0,0].annotate(f'Final RMSE: {RMSE[-1]:.1f}',
                  xy=(36, RMSE[-1]), xytext=(30, RMSE[-1]+2.5),
                  fontsize=fontsize, ha='right', color='blue')

# MAPE
axs[0,1].plot(epoch, MAPE, color='tab:orange', marker='s')
axs[0,1].set_title('MAPE (%) over Epochs', fontsize=fontsize+2)
axs[0,1].set_xlabel('Epoch', fontsize=fontsize)
axs[0,1].set_ylabel('MAPE (%)', fontsize=fontsize)
axs[0,1].grid(True)
axs[0,1].axvspan(15, 30, color='lightgreen', alpha=0.15)

axs[0,1].annotate('Steady decrease',
                  xy=(22, 25), xytext=(16, 29),
                  arrowprops=dict(facecolor='green', shrink=0.05),
                  fontsize=fontsize, color='green', ha='left')

axs[0,1].annotate(f'Lowest MAPE: {MAPE[-1]:.1f}%',
                  xy=(36, MAPE[-1]), xytext=(30, MAPE[-1]+3),
                  fontsize=fontsize, ha='right', color='darkorange')

axs[0,1].annotate(f'Early high: {MAPE[0]:.1f}%',
                  xy=(1, MAPE[0]), xytext=(5, MAPE[0]-3),
                  arrowprops=dict(facecolor='orange', shrink=0.05),
                  fontsize=fontsize, color='orange', ha='left')

# RÂ²
axs[1,0].plot(epoch, R2, color='tab:green', marker='^')
axs[1,0].set_title('RÂ² over Epochs', fontsize=fontsize+2)
axs[1,0].set_xlabel('Epoch', fontsize=fontsize)
axs[1,0].set_ylabel('RÂ²', fontsize=fontsize)
axs[1,0].set_ylim(0, 1)
axs[1,0].grid(True)
axs[1,0].axvspan(10, 25, color='lightblue', alpha=0.15)

axs[1,0].annotate('Rapid improvement',
                  xy=(20, 0.6), xytext=(10, 0.4),
                  arrowprops=dict(facecolor='blue', shrink=0.05),
                  fontsize=fontsize, color='blue', ha='left')

axs[1,0].annotate(f'Max RÂ²: {R2[-1]:.4f}',
                  xy=(36, R2[-1]), xytext=(30, 0.85),
                  fontsize=fontsize, color='darkgreen', ha='right')

axs[1,0].annotate(f'Start: {R2[0]:.2f}',
                  xy=(1, R2[0]), xytext=(5, R2[0]+0.12),
                  arrowprops=dict(facecolor='green', shrink=0.05),
                  fontsize=fontsize, color='green', ha='left')

# EVS
axs[1,1].plot(epoch, EVS, color='tab:red', marker='d')
axs[1,1].set_title('EVS over Epochs', fontsize=fontsize+2)
axs[1,1].set_xlabel('Epoch', fontsize=fontsize)
axs[1,1].set_ylabel('EVS', fontsize=fontsize)
axs[1,1].set_ylim(0, 1)
axs[1,1].grid(True)
axs[1,1].axvspan(20, 36, color='pink', alpha=0.15)

axs[1,1].annotate('Consistent growth',
                  xy=(30, 0.75), xytext=(22, 0.6),
                  arrowprops=dict(facecolor='red', shrink=0.05),
                  fontsize=fontsize, color='red', ha='left')

axs[1,1].annotate(f'Max EVS: {EVS[-1]:.3f}',
                  xy=(36, EVS[-1]), xytext=(30, 0.9),
                  fontsize=fontsize, color='darkred', ha='right')

axs[1,1].annotate(f'Start EVS: {EVS[0]:.2f}',
                  xy=(1, EVS[0]), xytext=(6, EVS[0]+0.12),
                  arrowprops=dict(facecolor='red', shrink=0.05),
                  fontsize=fontsize, color='darkred', ha='left')

plt.tight_layout()
plt.show()